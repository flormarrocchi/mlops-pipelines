{
 "cells": [
  {
   "cell_type": "raw",
   "id": "524fb204-9f6e-4f33-ae29-c6847dc1d38d",
   "metadata": {},
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fa935-7d99-4b42-ae3f-a457b5c4e070",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1edc2-5db8-4790-96f2-cbbb6c509d2a",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                                 google-cloud-storage \\\n",
    "                                 'kfp<2' \\\n",
    "                                 'google-cloud-pipeline-components<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43c045-839e-45f8-91a8-d3ce9f19e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b2041-e1bf-499b-9bc2-d59e8008d8d3",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "### Check the package versions\n",
    "\n",
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becf50c2-72f8-4f72-8c85-ff5e78711927",
   "metadata": {
    "id": "check_versions:kfp,gcpc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.22\n",
      "google_cloud_pipeline_components version: 1.0.44\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f75d0dc-ea5d-4257-85e6-682cef358044",
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"project-id\"  # @param {type:\"string\"}\n",
    "REGION = \"us-east1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e51a6b5-b05d-44fa-a807-325fd7fbdb0b",
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://fcustom-ml-experiments\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4bc88-822f-4e68-9b0f-56b83855cb24",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60bd8ca6-8edb-4c7c-aac8-9d57c9ebc809",
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 662741782935-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762a046-87f7-4bd2-bba1-075a4ebdc6b4",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant service account access to read and write pipeline artifacts in the bucket that is created in the previous step -- only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac52369b-b470-4a7c-9824-1e113bdd794b",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://fmcc-custom-ml-experiments/\n",
      "No changes made to gs://fmcc-custom-ml-experiments/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056abef-7f17-4700-a51b-72f985e8bce3",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a960c10-b83c-4843-b41c-2591846dbc85",
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics,\n",
    "                        Output, component, Dataset, Model, HTML, Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef65acc-3af4-4f03-bfea-25218dd02891",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### UUID\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cedbd0-818c-4156-a7b3-013a14cc4135",
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nqygikzk'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()\n",
    "UUID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4541d-0325-4304-b664-67e6ed4c2f45",
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "source": [
    "#### Vertex AI constants\n",
    "\n",
    "Setup up the following constants for Vertex AI Pipeline:\n",
    "- `PIPELINE_NAME`: Set name for the Pipeline.\n",
    "- `PIPELINE_ROOT`: Cloud Storage bucket path to store pipeline artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf517ac-e23c-400c-a567-ebcdf7726fcf",
   "metadata": {
    "id": "aip_constants:endpoint"
   },
   "outputs": [],
   "source": [
    "# set path for storing the pipeline artifacts\n",
    "PIPELINE_NAME = \"custom-ml-tabular-training\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/beans\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef59a9-b02b-405f-92d1-ec09b66aa55d",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f10f536-7146-4493-b6f5-556736e4e844",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a94f64-6dca-44c0-84ec-7b693415a27f",
   "metadata": {
    "id": "define_component:eval"
   },
   "source": [
    "## Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef009b46-96c5-4dc6-a515-a5cf4fabb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def _get_date() -> str:\n",
    "    \"\"\"Returns the current date.\"\"\"\n",
    "    import datetime  # pylint: disable=g-import-not-at-top,import-outside-toplevel\n",
    "    return datetime.datetime.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22cbb004-edbf-465e-9d60-ca1591e33ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    packages_to_install=[\"scorecardpy==0.1.9.6\"],\n",
    ")\n",
    "def credit_score_dataset(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import scorecardpy as sc\n",
    "\n",
    "    import logging\n",
    "\n",
    "    # load germancredit data\n",
    "    data = sc.germancredit()\n",
    "\n",
    "    # filter variable via missing rate, iv, identical value rate\n",
    "    dt_s = sc.var_filter(data, y=\"creditability\")\n",
    "\n",
    "    # breaking dt into train and test\n",
    "    train, test = sc.split_df(dt_s, 'creditability').values()\n",
    "    \n",
    "    # woe binning ------\n",
    "    bins = sc.woebin(dt_s, y=\"creditability\")\n",
    "    # sc.woebin_plot(bins)\n",
    "\n",
    "    # binning adjustment\n",
    "    # # adjust breaks interactively\n",
    "    # breaks_adj = sc.woebin_adj(dt_s, \"creditability\", bins) \n",
    "    # # or specify breaks manually\n",
    "    breaks_adj = {\n",
    "        'age.in.years': [26, 35, 40],\n",
    "        'other.debtors.or.guarantors': [\"none\", \"co-applicant%,%guarantor\"]\n",
    "    }\n",
    "    bins_adj = sc.woebin(dt_s, y=\"creditability\", breaks_list=breaks_adj)\n",
    "    \n",
    "    # converting train and test into woe values\n",
    "    train_woe = sc.woebin_ply(train, bins_adj)\n",
    "    test_woe = sc.woebin_ply(test, bins_adj)\n",
    "\n",
    "    train_woe.to_csv(dataset_train.path, index=False)\n",
    "    test_woe.to_csv(dataset_test.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089bb272-8f30-4f6b-8445-0366ec9e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    packages_to_install=[\"scorecardpy==0.1.9.6\"],\n",
    ")\n",
    "def model_train(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Artifact],\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    train_woe = pd.read_csv(dataset.path)\n",
    "    y_train = train_woe.loc[:,'creditability']\n",
    "    X_train = train_woe.loc[:,train_woe.columns != 'creditability']\n",
    "\n",
    "    model_pipeline =  LogisticRegression(penalty='l1', C=0.9, solver='saga', n_jobs=-1, random_state=42)\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    }\n",
    "\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    import pathlib\n",
    "\n",
    "    pathlib.Path(model.path).mkdir()\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(model_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6af607ac-7184-4729-bba9-df95c6ac6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    packages_to_install=[\"scorecardpy==0.1.9.6\"],\n",
    ")\n",
    "def model_evaluate_metric(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    # metricsc: Output[ClassificationMetrics],\n",
    ") -> dict:\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import (roc_curve,\n",
    "                                 confusion_matrix,\n",
    "                                 accuracy_score,\n",
    "                                 precision_score,\n",
    "                                 recall_score,\n",
    "                                 f1_score,\n",
    "                                 log_loss,\n",
    "                                 roc_auc_score,\n",
    "                                 average_precision_score)\n",
    "    \n",
    "    data = pd.read_csv(test_set.path)\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "    \n",
    "    X=data.drop(columns=['creditability'])\n",
    "    y=data.creditability\n",
    "\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "#     metricsc.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "#     metricsc.log_confusion_matrix(\n",
    "#         [\"good\", \"bad\"],\n",
    "#         confusion_matrix(y, y_pred).tolist(),\n",
    "#     )\n",
    "    \n",
    "    metrics.log_metric('Framework', 'scikit-learn')\n",
    "    metrics.log_metric('Threshold','0.5000')\n",
    "    metrics.log_metric('Precision', precision_score(y, y_pred))\n",
    "    metrics.log_metric('Recall', recall_score(y, y_pred))\n",
    "    metrics.log_metric('Accuracy', accuracy_score(y, y_pred))\n",
    "    metrics.log_metric('F1 score', f1_score(y, y_pred))\n",
    "    metrics.log_metric('Log loss', log_loss(y, y_pred))\n",
    "    metrics.log_metric('ROC AUC', roc_auc_score(y, y_scores))\n",
    "    metrics.log_metric('ROC PR', average_precision_score(y, y_scores))\n",
    "    \n",
    "    output = {'auROC': roc_auc_score(y, y_pred)}\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d16a50-47d5-4bd2-9cb0-1f44339c770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\",\n",
    "    packages_to_install=[\"scorecardpy==0.1.9.6\"],\n",
    ")\n",
    "def model_evaluate_matrix(\n",
    "    test_set: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> dict:\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.metrics import (roc_curve,\n",
    "                                 confusion_matrix,\n",
    "                                 accuracy_score,\n",
    "                                )\n",
    "    \n",
    "    data = pd.read_csv(test_set.path)\n",
    "    file_name = model.path + \"/model.pkl\"\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        model_pipeline = pickle.load(file)\n",
    "    \n",
    "    X=data.drop(columns=['creditability'])\n",
    "    y=data.creditability\n",
    "\n",
    "    y_pred = model_pipeline.predict(X)\n",
    "\n",
    "    y_scores = model_pipeline.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=y_scores, pos_label=True)\n",
    "    metricsc.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metricsc.log_confusion_matrix(\n",
    "        [\"good\", \"bad\"],\n",
    "        confusion_matrix(y, y_pred).tolist(),\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be97b9-3619-4f57-89eb-46a07b7b9aba",
   "metadata": {
    "id": "define_pipeline:gcpc,beans,lcn"
   },
   "source": [
    "## Define pipeline \n",
    "\n",
    "Define the pipeline for AutoML tabular classification using the components from `google_cloud_pipeline_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e29ebcce-1616-48b5-8d81-ca5b7a86fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    UUID: str\n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "        create_custom_training_job_op_from_component,\n",
    "    )\n",
    "    \n",
    "    data_op = credit_score_dataset()\n",
    "\n",
    "    custom_job_distributed_training_op = create_custom_training_job_op_from_component(\n",
    "        model_train,\n",
    "        replica_count=1\n",
    "    )\n",
    "\n",
    "    model_train_op = custom_job_distributed_training_op(\n",
    "        dataset=data_op.outputs[\"dataset_train\"],\n",
    "        project=project,\n",
    "        location=location,\n",
    "    ).after(data_op)\n",
    "\n",
    "    model_evaluate_metric_op = model_evaluate_metric(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        model=model_train_op.outputs[\"model\"],\n",
    "    ).after(model_train_op)\n",
    "    \n",
    "    # model_evaluate_matrix_op = model_evaluate_matrix(\n",
    "    #     test_set=data_op.outputs[\"dataset_test\"],\n",
    "    #     model=model_train_op.outputs[\"model\"],\n",
    "    # ).after(model_train_op)\n",
    "    \n",
    "    # shapely parameters\n",
    "    parameters = {\"sampled_shapley_attribution\": {\"path_count\": 10}}\n",
    "    \n",
    "    # Explanation metadata\n",
    "    COLUMNS = ['purpose_woe', 'installment_rate_in_percentage_of_disposable_income_woe', 'status_of_existing_checking_account_woe', 'housing_woe', 'credit_history_woe', 'savings_account_and_bonds_woe', 'duration_in_month_woe', 'present_employment_since_woe', 'age_in_years_woe', 'other_debtors_or_guarantors_woe', 'other_installment_plans_woe', 'property_woe', 'credit_amount_woe']\n",
    "\n",
    "    metadata = {\n",
    "    \"inputs\":{\n",
    "        \"features\": {\"index_feature_mapping\": COLUMNS, \"encoding\": \"BAG_OF_FEATURES\"}\n",
    "    },\n",
    "    \"outputs\":{\"creditability\": {}}}\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"german-credit-scroe-model-{UUID}\",\n",
    "        unmanaged_container_model=model_train_op.outputs[\"model\"],\n",
    "        explanation_parameters=parameters,\n",
    "        explanation_metadata=metadata,\n",
    "    ).after(model_train_op)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        display_name=f\"german-credit-scroe-endpoint-{UUID}\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=f\"german-credit-scroe-model-{UUID}\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    ).after(model_upload_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfbc15-f0e7-48e0-be7f-fa2411f82efd",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline to the specified json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af25bec-2b94-4898-9c19-bcc5d3beb089",
   "metadata": {
    "id": "compile_pipeline"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"custom_classification_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965a7de-6164-461c-a62e-d497ede9b6f9",
   "metadata": {
    "id": "run_pipeline:model"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Pass the input parameters required for the pipeline and run it. The defined pipeline takes the following parameters:\n",
    "- `project`: Project-id where the pipeline is run.\n",
    "- `location`: Region for setting the pipeline location.\n",
    "- `UUID`: uuid for current instance session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e5f1320-c5ec-4fde-88ef-d7ac7c9ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": REGION,\n",
    "    \"UUID\": UUID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb0b2ef2-6bb1-4df6-b463-6ef44dd1d27d",
   "metadata": {
    "id": "run_pipeline:model"
   },
   "outputs": [],
   "source": [
    "# Configure the pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"custom_classification_training_{UUID}\",\n",
    "    template_path=\"custom_classification_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acc2ac-0edb-4be3-be92-c69e9c7a338b",
   "metadata": {
    "id": "view_pipeline_run:model"
   },
   "source": [
    "Run the pipeline job. Click on the generated link to see your run in the Cloud Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ece65e7-370a-4494-b3f3-a0c0492fc02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nqygikzk'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UUID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab925631-d376-4b1a-9115-47760c5c606a",
   "metadata": {
    "id": "114ab8ff24ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-ml-tabular-training-20231002115217?project=662741782935\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/662741782935/locations/us-central1/pipelineJobs/custom-ml-tabular-training-20231002115217\n"
     ]
    }
   ],
   "source": [
    "# Run the job\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b34128-f355-46cc-bd95-90c41d0b7d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa5fe3-ede1-4d68-b8ef-3cf3c245d0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
